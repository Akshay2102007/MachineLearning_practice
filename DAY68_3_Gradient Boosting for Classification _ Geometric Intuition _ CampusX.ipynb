{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxHRyKxYxX2jSQzXKUAiVa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"rdeYLRcrPviP","executionInfo":{"status":"ok","timestamp":1750241542400,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akshay Pratap Singh","userId":"13893775229215889614"}}},"outputs":[],"source":["import numpy as np\n","from sklearn.tree import DecisionTreeRegressor\n","class CustomGradientBoostingClassifier:\n","\n","    def __init__(self, learning_rate, n_estimators, max_depth=1):\n","        self.learning_rate = learning_rate\n","        self.n_estimators = n_estimators\n","        self.max_depth = max_depth\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","\n","        F0 = np.log(y.mean()/(1-y.mean()))  # log-odds values\n","        self.F0 = np.full(len(y), F0)  # converting to array with the input length\n","        Fm = self.F0.copy()\n","\n","        for _ in range(self.n_estimators):\n","            p = np.exp(Fm) / (1 + np.exp(Fm))  # converting back to probabilities\n","            r = y - p  # residuals\n","            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=0)\n","            tree.fit(X, r)\n","            ids = tree.apply(x)  # getting the terminal node IDs\n","\n","            # looping through the terminal nodes\n","            for j in np.unique(ids):\n","              fltr = ids == j\n","\n","              # getting gamma using the formula (Σresiduals/Σp(1-p))\n","              num = r[fltr].sum()\n","              den = (p[fltr]*(1-p[fltr])).sum()\n","              gamma = num / den\n","\n","              # updating the prediction\n","              Fm[fltr] += self.learning_rate * gamma\n","\n","              # replacing the prediction value in the tree\n","              tree.tree_.value[j, 0, 0] = gamma\n","\n","            self.trees.append(tree)\n","\n","    def predict_proba(self, X):\n","\n","        Fm = self.F0\n","\n","        for i in range(self.n_estimators):\n","            Fm += self.learning_rate * self.trees[i].predict(X)\n","\n","        return np.exp(Fm) / (1 + np.exp(Fm))  # converting back into probabilities"]},{"cell_type":"code","source":["# Generate some dummy data for demonstration\n","from sklearn.datasets import make_classification\n","\n","x, y = make_classification(n_samples=100, n_features=20, random_state=0)"],"metadata":{"id":"MKtn-PtQh-t3","executionInfo":{"status":"ok","timestamp":1750241639879,"user_tz":-330,"elapsed":3,"user":{"displayName":"Akshay Pratap Singh","userId":"13893775229215889614"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\"\"\"Next, we are checking if our CustomGradientBoostingClassifier performs as\n","the same as GradientBoostingClassifier from scikit-learn by looking at their\n"," log-loss on our data.\"\"\"\n","\n","\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.metrics import log_loss\n","\n","custom_gbm = CustomGradientBoostingClassifier(\n","    n_estimators=20,\n","    learning_rate=0.1,\n","    max_depth=1\n",")\n","custom_gbm.fit(x, y)\n","custom_gbm_log_loss = log_loss(y, custom_gbm.predict_proba(x))\n","print(f\"Custom GBM Log-Loss:{custom_gbm_log_loss:.15f}\")\n","\n","sklearn_gbm = GradientBoostingClassifier(\n","    n_estimators=20,\n","    learning_rate=0.1,\n","    max_depth=1\n",")\n","sklearn_gbm.fit(x, y)\n","sklearn_gbm_log_loss = log_loss(y, sklearn_gbm.predict_proba(x))\n","print(f\"Scikit-learn GBM Log-Loss:{sklearn_gbm_log_loss:.15f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VSF7FmqhDau","executionInfo":{"status":"ok","timestamp":1750241641900,"user_tz":-330,"elapsed":10,"user":{"displayName":"Akshay Pratap Singh","userId":"13893775229215889614"}},"outputId":"927ebaf1-ed4c-4468-bb77-f52718a5c2d9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom GBM Log-Loss:0.255023869100879\n","Scikit-learn GBM Log-Loss:0.255023869100879\n"]}]}]}